<!DOCTYPE html><html style="display:none" lang="zh"><head><meta charset="utf-8"><script>window.materialVersion="1.5.0",window.oldVersion=["codestartv1","1.3.4","1.4.0","1.4.0b1"]</script><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://busuanzi.ibruce.info"><link rel="dns-prefetch" href="https://changyan.sohu.com"><title> DNN深度神经网络的损失函数和激活函数 | Edward&#39;s Blog</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0097A7"><meta name="author" content="Edward Guan"><meta name="description" itemprop="description" content="深度神经网络（DNN）损失函数和激活函数的选择"><meta name="keywords" content="前端,node,react,js,java,自学编程,学习分享,UESTC,深度学习"><script>window.lsloader={jsRunSequence:[],jsnamemap:{},cssnamemap:{}},lsloader.removeLS=function(e){try{localStorage.removeItem(e)}catch(e){}},lsloader.setLS=function(e,t){try{localStorage.setItem(e,t)}catch(e){}},lsloader.getLS=function(e){var t="";try{t=localStorage.getItem(e)}catch(e){t=""}return t},versionString="/*"+(window.materialVersion||"unknownVersion")+"*/",lsloader.clean=function(){try{for(var e=[],t=0;t<localStorage.length;t++)e.push(localStorage.key(t));e.forEach(function(e){var t=lsloader.getLS(e);window.oldVersion&&window.oldVersion.reduce(function(e,n){return e||-1!==t.indexOf("/*"+n+"*/")},!1)&&lsloader.removeLS(e)})}catch(e){}},lsloader.clean(),lsloader.load=function(e,t,n,s){"boolean"==typeof n&&(s=n,n=void 0),s=s||!1,n=n||function(){};var a;if((a=this.getLS(e))&&-1===a.indexOf(versionString))return this.removeLS(e),void this.requestResource(e,t,n,s);if(a){if(a.split(versionString)[0]!=t)return console.log("reload:"+t),this.removeLS(e),void this.requestResource(e,t,n,s);a=a.split(versionString)[1],s?(this.jsRunSequence.push({name:e,code:a}),this.runjs(t,e,a)):(document.getElementById(e).appendChild(document.createTextNode(a)),n())}else this.requestResource(e,t,n,s)},lsloader.requestResource=function(e,t,n,s){var a=this;s?this.iojs(t,e,function(e,t,n){a.setLS(t,e+versionString+n),a.runjs(e,t,n)}):this.iocss(t,e,function(n){document.getElementById(e).appendChild(document.createTextNode(n)),a.setLS(e,t+versionString+n)},n)},lsloader.iojs=function(e,t,n){var s=this;s.jsRunSequence.push({name:t,code:""});try{var a=new XMLHttpRequest;a.open("get",e,!0),a.onreadystatechange=function(){if(4==a.readyState){if((a.status>=200&&a.status<300||304==a.status)&&""!=a.response)return void n(e,t,a.response);s.jsfallback(e,t)}},a.send(null)}catch(n){s.jsfallback(e,t)}},lsloader.iocss=function(e,t,n,s){var a=this;try{var o=new XMLHttpRequest;o.open("get",e,!0),o.onreadystatechange=function(){if(4==o.readyState){if((o.status>=200&&o.status<300||304==o.status)&&""!=o.response)return n(o.response),void s();a.cssfallback(e,t,s)}},o.send(null)}catch(n){a.cssfallback(e,t,s)}},lsloader.iofonts=function(e,t,n,s){var a=this;try{var o=new XMLHttpRequest;o.open("get",e,!0),o.onreadystatechange=function(){if(4==o.readyState){if((o.status>=200&&o.status<300||304==o.status)&&""!=o.response)return n(o.response),void s();a.cssfallback(e,t,s)}},o.send(null)}catch(n){a.cssfallback(e,t,s)}},lsloader.runjs=function(e,t,n){if(t&&n)for(var s in this.jsRunSequence)this.jsRunSequence[s].name==t&&(this.jsRunSequence[s].code=n);if(this.jsRunSequence[0]&&this.jsRunSequence[0].code&&"failed"!=this.jsRunSequence[0].status)(o=document.createElement("script")).appendChild(document.createTextNode(this.jsRunSequence[0].code)),o.type="text/javascript",document.getElementsByTagName("head")[0].appendChild(o),this.jsRunSequence.shift(),this.jsRunSequence.length>0&&this.runjs();else if(this.jsRunSequence[0]&&"failed"==this.jsRunSequence[0].status){var a=this,o=document.createElement("script");o.src=this.jsRunSequence[0].path,o.type="text/javascript",this.jsRunSequence[0].status="loading",o.onload=function(){a.jsRunSequence.shift(),a.jsRunSequence.length>0&&a.runjs()},document.body.appendChild(o)}},lsloader.tagLoad=function(e,t){this.jsRunSequence.push({name:t,code:"",path:e,status:"failed"}),this.runjs()},lsloader.jsfallback=function(e,t){if(!this.jsnamemap[t]){this.jsnamemap[t]=t;for(var n in this.jsRunSequence)this.jsRunSequence[n].name==t&&(this.jsRunSequence[n].code="",this.jsRunSequence[n].status="failed",this.jsRunSequence[n].path=e);this.runjs()}},lsloader.cssfallback=function(e,t,n){if(!this.cssnamemap[t]){this.cssnamemap[t]=1;var s=document.createElement("link");s.type="text/css",s.href=e,s.rel="stylesheet",s.onload=s.onerror=n;var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(s,a)}},lsloader.runInlineScript=function(e,t){var n=document.getElementById(t).innerText;this.jsRunSequence.push({name:e,code:n}),this.runjs()},lsloader.loadCombo=function(e){var t="",n={};for(var s in e){var a=this.getLS(e[s].name);if(a)var o=a.split(versionString)[0],i=a.split(versionString)[1];else o="";o==e[s].path?this.jsRunSequence.push({name:e[s].name,code:i,path:e[s].path}):(this.jsRunSequence.push({name:e[s].name,code:null,path:e[s].path,status:"comboloading"}),n[e[s].name]=!0,t+=(""==t?"":";")+e[s].path)}var u=this;if(t){var r=new XMLHttpRequest;r.open("get",combo+t,!0),r.onreadystatechange=function(){if(4==r.readyState)if(r.status>=200&&r.status<300||304==r.status){if(""!=r.response)return void u.runCombo(r.response,n)}else{for(var e in u.jsRunSequence)n[u.jsRunSequence[e].name]&&(u.jsRunSequence[e].status="failed");u.runjs()}},r.send(null)}this.runjs()},lsloader.runCombo=function(e,t){(e=e.split("/*combojs*/")).shift();for(var n in this.jsRunSequence)t[this.jsRunSequence[n].name]&&e[0]&&(this.jsRunSequence[n].status="comboJS",this.jsRunSequence[n].code=e[0],this.setLS(this.jsRunSequence[n].name,this.jsRunSequence[n].path+versionString+e[0]),e.shift());this.runjs()}</script><script>function Queue(){this.dataStore=[],this.offer=function(e){this.debug&&console.log("Offered a Queued Function."),"function"==typeof e?this.dataStore.push(e):console.log("You must offer a function.")},this.poll=function(){return this.debug&&console.log("Polled a Queued Function."),this.dataStore.shift()},this.execNext=function(){var e=this.poll();void 0!==e&&(this.debug&&console.log("Run a Queued Function."),e())},this.debug=!1,this.startDebug=function(){this.debug=!0}}var queue=new Queue</script><link rel="icon shortcut" type="image/ico" href="http://ornavpdfn.bkt.clouddn.com/blogCDN/icon-crown.svg"><link rel="icon" sizes="192x192" href="http://ornavpdfn.bkt.clouddn.com/blogCDN/icon-crown.svg"><link rel="apple-touch-icon" href="http://ornavpdfn.bkt.clouddn.com/blogCDN/icon-crown.svg"><meta name="apple-mobile-web-app-title" content="Title"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="480"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="apple-mobile-web-app-title" content="Edward&#39;s Blog"> <!--[if lte IE 9]><link rel="stylesheet" href="/css/ie-blocker.css"><script src="/js/ie-blocker.zhCN.js"></script><![endif]--><style id="material_css"></style><script>void 0===window.lsLoadCSSMaxNums&&(window.lsLoadCSSMaxNums=0),window.lsLoadCSSMaxNums++,lsloader.load("material_css","/css/material.min.css?Z7a72R1E4SxzBKR/WGctOA==",function(){void 0===window.lsLoadCSSNums&&(window.lsLoadCSSNums=0),window.lsLoadCSSNums++,window.lsLoadCSSNums==window.lsLoadCSSMaxNums&&(document.documentElement.style.display="")},!1)</script><style id="style_css"></style><script>void 0===window.lsLoadCSSMaxNums&&(window.lsLoadCSSMaxNums=0),window.lsLoadCSSMaxNums++,lsloader.load("style_css","/css/style.min.css?MKetZV3cUTfDxvMffaOezg==",function(){void 0===window.lsLoadCSSNums&&(window.lsLoadCSSNums=0),window.lsLoadCSSNums++,window.lsLoadCSSNums==window.lsLoadCSSMaxNums&&(document.documentElement.style.display="")},!1)</script><style id="prettify_css"></style><script>void 0===window.lsLoadCSSMaxNums&&(window.lsLoadCSSMaxNums=0),window.lsLoadCSSMaxNums++,lsloader.load("prettify_css","/css/prettify.min.css?zp8STOU9v89XWFEnN+6YmQ==",function(){void 0===window.lsLoadCSSNums&&(window.lsLoadCSSNums=0),window.lsLoadCSSNums++,window.lsLoadCSSNums==window.lsLoadCSSMaxNums&&(document.documentElement.style.display="")},!1)</script><style id="prettify_theme"></style><script>void 0===window.lsLoadCSSMaxNums&&(window.lsLoadCSSMaxNums=0),window.lsLoadCSSMaxNums++,lsloader.load("prettify_theme","/css/prettify/github-v2.min.css?AfzKxt++K+/lhZBlSjnxwg==",function(){void 0===window.lsLoadCSSNums&&(window.lsLoadCSSNums=0),window.lsLoadCSSNums++,window.lsLoadCSSNums==window.lsLoadCSSMaxNums&&(document.documentElement.style.display="")},!1)</script><style>body,html{font-family:Roboto,"Helvetica Neue",Helvetica,"PingFang SC","Hiragino Sans GB","Microsoft YaHei","微软雅黑",Arial,sans-serif;overflow-x:hidden!important}code{font-family:Consolas,Monaco,'Andale Mono','Ubuntu Mono',monospace}a{color:#00838f}#scheme-Paradox .hot_tags-count,#scheme-Paradox .sidebar-colored .sidebar-badge,#scheme-Paradox .sidebar-colored .sidebar-header,#scheme-Paradox .sidebar_archives-count,#search-form-label:after,#search-label,.mdl-card__media{background-color:#0097a7!important}#scheme-Paradox .sidebar-colored .sidebar-nav>.dropdown>.dropdown-menu>li>a:focus,#scheme-Paradox .sidebar-colored .sidebar-nav>.dropdown>.dropdown-menu>li>a:hover{color:#0097a7!important}#ds-reset #ds-ctx .ds-ctx-entry .ds-ctx-head a,#post_entry-right-info,.sidebar-colored .sidebar-nav li:hover>a,.sidebar-colored .sidebar-nav li:hover>a i,.sidebar-colored .sidebar-nav li>a:focus i,.sidebar-colored .sidebar-nav li>a:hover,.sidebar-colored .sidebar-nav li>a:hover i,.sidebar-colored .sidebar-nav>.open>a,.sidebar-colored .sidebar-nav>.open>a:focus,.sidebar-colored .sidebar-nav>.open>a:hover{color:#0097a7!important}.toTop{background:#757575!important}.material-layout .material-index>.material-nav,.material-layout .material-post>.material-nav,.material-nav a{color:#757575}#scheme-Paradox .MD-burger-layer{background-color:#757575}#scheme-Paradox #post-toc-trigger-btn{color:#757575}.post-toc a:hover{color:#00838f;text-decoration:underline}</style><style>body{background-color:#f5f5f5}#scheme-Paradox .material-layout .something-else .mdl-card__supporting-text{background-color:#fff}</style><style>.fade{transition:all .8s linear;-webkit-transform:translate3d(0,0,0);-moz-transform:translate3d(0,0,0);-ms-transform:translate3d(0,0,0);-o-transform:translate3d(0,0,0);transform:translate3d(0,0,0);opacity:1}.fade.out{opacity:0}</style><link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" rel="stylesheet"><link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet"><script>lsloader.load("jq_js","/js/jquery.min.js?qcusAULNeBksqffqUM2+Ig==",!0)</script><meta property="og:url" content="http://pancakeawesome.ink"><meta property="og:type" content="blog"><meta property="og:title" content="DNN深度神经网络的损失函数和激活函数 | Edward&#39;s Blog"><meta property="og:image" content="http://pancakeawesome.inkhttp://ornavpdfn.bkt.clouddn.com/blogCDN/icon-crown.svg"><meta property="og:description" content="深度神经网络（DNN）损失函数和激活函数的选择"><meta property="og:article:tag" content="深度学习"><meta property="article:published_time" content="Thu Mar 08 2018 11:24:23 GMT+0800"><meta property="article:modified_time" content="Tue Apr 10 2018 22:37:01 GMT+0800"><meta name="twitter:title" content="DNN深度神经网络的损失函数和激活函数 | Edward&#39;s Blog"><meta name="twitter:description" content="深度神经网络（DNN）损失函数和激活函数的选择"><meta name="twitter:image" content="http://pancakeawesome.inkhttp://ornavpdfn.bkt.clouddn.com/blogCDN/icon-crown.svg"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="http://pancakeawesome.ink"><link rel="canonical" href="http://pancakeawesome.ink/深度神经网络损失函数和激活函数的选择.html"><script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "mainEntityOfPage": "http://pancakeawesome.ink/深度神经网络损失函数和激活函数的选择.html",
    "headline": "DNN深度神经网络的损失函数和激活函数",
    "datePublished": "Thu Mar 08 2018 11:24:23 GMT+0800",
    "dateModified": "Tue Apr 10 2018 22:37:01 GMT+0800",
    "author": {
        "@type": "Person",
        "name": "Edward Guan",
        "image": {
            "@type": "ImageObject",
            "url": "/img/rip.jpeg"
        },
        "description": "你说人生艳丽我没有异议，你说人生忧郁我不言语"
    },
    "publisher": {
        "@type": "Organization",
        "name": "Edward&#39;s Blog",
        "logo": {
            "@type":"ImageObject",
            "url": "http://ornavpdfn.bkt.clouddn.com/blogCDN/icon-crown.svg"
        }
    },
    "keywords": ",深度学习前端,node,react,js,java,自学编程,学习分享,UESTC",
    "description": "深度神经网络（DNN）损失函数和激活函数的选择",
}
</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0c041dca6f79c4e40fb45d1283e327e7";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></head><body id="scheme-Paradox" class="lazy"><div class="material-layout mdl-js-layout has-drawer is-upgraded"><main class="material-layout__content" id="main"><div id="top"></div> <button class="MD-burger-icon sidebar-toggle"><span class="MD-burger-layer"></span></button> <button id="post-toc-trigger-btn" class="mdl-button mdl-js-button mdl-button--icon"> <i class="material-icons">format_list_numbered</i></button><ul class="post-toc-wrap mdl-menu mdl-menu--bottom-left mdl-js-menu mdl-js-ripple-effect" for="post-toc-trigger-btn" style="max-height:80vh;overflow-y:scroll"><ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#前言"><span class="post-toc-number">1.</span> <span class="post-toc-text">前言</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#深度神经网络（DNN）损失函数和激活函数的选择"><span class="post-toc-number">2.</span> <span class="post-toc-text">深度神经网络（DNN）损失函数和激活函数的选择</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1-均方差损失函数-Sigmoid激活函数的问题"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">1. 均方差损失函数+Sigmoid激活函数的问题</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-使用交叉熵损失函数-Sigmoid激活函数改进DNN算法收敛速度"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">2. 使用交叉熵损失函数+Sigmoid激活函数改进DNN算法收敛速度</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-使用对数似然损失函数和softmax激活函数进行DNN分类输出"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">3. 使用对数似然损失函数和softmax激活函数进行DNN分类输出</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-梯度爆炸梯度消失与ReLU激活函数"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">4. 梯度爆炸梯度消失与ReLU激活函数</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#5-DNN其他激活函数"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">5. DNN其他激活函数</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#后记"><span class="post-toc-number">3.</span> <span class="post-toc-text">后记</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#DNN损失函数和激活函数小结"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">DNN损失函数和激活函数小结</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#参考资料："><span class="post-toc-number">3.2.</span> <span class="post-toc-text">参考资料：</span></a></li></ol></li></ol></ul><div class="material-post_container"><div class="material-post mdl-grid"><div class="mdl-card mdl-shadow--4dp mdl-cell mdl-cell--12-col"><div class="post_thumbnail-random mdl-card__media mdl-color-text--grey-50"><script type="text/ls-javascript" id="post-thumbnail-script">
    var randomNum = Math.floor(Math.random() * 30 + 1);

    $('.post_thumbnail-random').attr('data-original', '/img/random/material-' + randomNum + '.png');
    $('.post_thumbnail-random').addClass('lazy');
</script><p class="article-headline-p"> DNN深度神经网络的损失函数和激活函数</p></div><div class="mdl-color-text--grey-700 mdl-card__supporting-text meta"><div id="author-avatar"> <img src="/img/rip.jpeg" width="44px" height="44px" alt="Author Avatar"></div><div> <strong>Edward Guan</strong> <span>3月 08, 2018</span></div><div class="section-spacer"></div> <button id="article-functions-qrcode-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon"> <i class="material-icons" role="presentation">devices other</i> <span class="visuallyhidden">devices other</span></button><ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-functions-qrcode-button"><li class="mdl-menu__item">在其它设备中阅读本文章</li> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQ4AAAEOCAAAAABd2qZ5AAADhklEQVR42u3aS27dQAwEQN//0sk2QGK9bnIkK0BpZTzoN6VFmxx+/XL8cXwhwIEDBw4cOHDgwIHjxRxf8fHdVf94wOVV3/2S3zM/J1oLDhw4cOC4geNDFAUvnfDlWNfLyLmv14IDBw4cOO7muH78dVhuovf6ifvP9uHOOHDgwIHjNRxJadeeM1s2Dhw4cOD4fznauE2adLOITe6DAwcOHDh+liNvqOWbPXnjL6F8Xa8UBw4cOHCsBxre//ej8x04cODAgeMvjvbI4zBv/M2uvd7oKlaEAwcOHDiOcmy2edoFbwYR8oithy1w4MCBA8dRjs2tkzP3Id3+fuD/Dhw4cODAcYhj87rJ6EAbgbMzceDAgQPHezjOlmRR+bTY9GqbmN+WoDhw4MCB4yhHW2jlIdcWe/k2UjuysNp2woEDBw4cC47Nsocht27ztRtaOHDgwIHjeY42dPNlbwI+GQcfhj0OHDhw4DjKMWsOtsG8Weqm2Hto2wkHDhw4cMQlXBJvLUHy9HbAog1gHDhw4MDxDMcsRPOrEo6zrcCozMOBAwcOHEc52lZge1U79LYaTQjiOapoceDAgQPHgqMdgG6bdJvxiA1xXnbiwIEDB46zHG051x55K7AtCzfX4sCBAweO+zjadlteaG0iMx9lmBHgwIEDB467OdrRgTxQ93drhy1w4MCBA8c7OdoRgdn20mwLqh2/+/AUHDhw4MBxG0dbOLVRNwzCOM7zRicOHDhw4LiPIz8SgiRQZ22+WahHv+DAgQMHjqMc7U03wZyH4oygbVniwIEDB447ONpY2rfw2tZh/tlaShw4cODA8QzHZqytLdvaraO2mMSBAwcOHM9ztNs/+aDDbNNoE8/tKnDgwIEDx30cebstH0eYDd61/wS0H2/VK8WBAwcOHDFHHr2zRc7G6doisG0L4sCBAweOJzlODQ3MmoNn25cfWo04cODAgeMox4HmWhl77dZUMrhwrITDgQMHDhxHm4PFeNno/GQcYRP2wyE8HDhw4MBxA8c+XNugbRuO+bu174ADBw4cOO7jmBVRyTLapmFy/izyi20nHDhw4MDxIEf+sHazqn3W5pPgwIEDB473c1w3+NpNqXZrqo1YHDhw4MBxN0fbgMsLvOQVN4N0CdCN2044cODAgWMx0NC2/2ZLymk2zU0cOHDgwHEHhwMHDhw4cODAgQMHDhwvO34DPJrjLIfIf9sAAAAASUVORK5CYII="></ul> <button id="article-functions-viewtags-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon"> <i class="material-icons" role="presentation">bookmark</i> <span class="visuallyhidden">bookmark</span></button><ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-functions-viewtags-button"><li class="mdl-menu__item"> <a class="post_tag-link" href="/tags/深度学习/">深度学习</a></li></ul> <button id="article-fuctions-share-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon"> <i class="material-icons" role="presentation">share</i> <span class="visuallyhidden">share</span></button><ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-fuctions-share-button"><a class="post_share-link" href="#"><li class="mdl-menu__item"><span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span> &nbsp;浏览量</span></li></a><a class="post_share-link" href="http://service.weibo.com/share/share.php?appkey=&title=DNN深度神经网络的损失函数和激活函数&url=http://pancakeawesome.ink/深度神经网络损失函数和激活函数的选择.html&pic=http://pancakeawesome.inkhttp://ornavpdfn.bkt.clouddn.com/blogCDN/icon-crown.svg&searchPic=false&style=simple" target="_blank"><li class="mdl-menu__item"> 分享到微博</li></a><a class="post_share-link" href="https://twitter.com/intent/tweet?text=DNN深度神经网络的损失函数和激活函数&url=http://pancakeawesome.ink/深度神经网络损失函数和激活函数的选择.html&via=Edward Guan" target="_blank"><li class="mdl-menu__item"> 分享到 Twitter</li></a><a class="post_share-link" href="https://www.facebook.com/sharer/sharer.php?u=http://pancakeawesome.ink/深度神经网络损失函数和激活函数的选择.html" target="_blank"><li class="mdl-menu__item"> 分享到 Facebook</li></a><a class="post_share-link" href="https://plus.google.com/share?url=http://pancakeawesome.ink/深度神经网络损失函数和激活函数的选择.html" target="_blank"><li class="mdl-menu__item"> 分享到 Google+</li></a><a class="post_share-link" href="https://www.linkedin.com/shareArticle?mini=true&url=http://pancakeawesome.ink/深度神经网络损失函数和激活函数的选择.html&title=DNN深度神经网络的损失函数和激活函数" target="_blank"><li class="mdl-menu__item"> 分享到 LinkedIn</li></a><a class="post_share-link" href="http://connect.qq.com/widget/shareqq/index.html?site=Edward&#39;s Blog&title=DNN深度神经网络的损失函数和激活函数&summary=技术分享,学习笔记&pics=http://pancakeawesome.inkhttp://ornavpdfn.bkt.clouddn.com/blogCDN/icon-crown.svg&url=http://pancakeawesome.ink/深度神经网络损失函数和激活函数的选择.html" target="_blank"><li class="mdl-menu__item"> 分享到 QQ</li></a></ul></div><div id="post-content" class="mdl-color-text--grey-700 mdl-card__supporting-text fade out"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li><a href="http://pancakeawesome.ink/深度神经网络(DNN)模型与前向传播算法(FP).html">深度神经网络和前向传播算法</a></li></ul><ul><li><a href="http://pancakeawesome.ink/深度神经网络(DNN)与反向传播算法(BP).html">深度神经网络和反向传播算法</a></li></ul><ul><li><a href="http://pancakeawesome.ink/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88DNN%EF%BC%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9.html">深度神经网络的损失函数和激活函数</a></li><li><a href="http://pancakeawesome.ink/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96.html">深度神经网络的正则化</a></li></ul><p>在<a href="http://pancakeawesome.ink/深度神经网络(DNN)与反向传播算法(BP).html">深度神经网络（DNN）反向传播算法(BP)</a> 中，我们对DNN的前向反向传播算法的使用做了总结。里面使用的损失函数是均方差，而激活函数是Sigmoid。实际上DNN可以使用的损失函数和激活函数不少。这些损失函数和激活函数如何选择呢？下面我们就对DNN损失函数和激活函数的选择做一个总结。</p><h2 id="深度神经网络（DNN）损失函数和激活函数的选择"><a href="#深度神经网络（DNN）损失函数和激活函数的选择" class="headerlink" title="深度神经网络（DNN）损失函数和激活函数的选择"></a>深度神经网络（DNN）损失函数和激活函数的选择</h2><h3 id="1-均方差损失函数-Sigmoid激活函数的问题"><a href="#1-均方差损失函数-Sigmoid激活函数的问题" class="headerlink" title="1. 均方差损失函数+Sigmoid激活函数的问题"></a>1. 均方差损失函数+Sigmoid激活函数的问题</h3><p>在讲反向传播算法时，我们用均方差损失函数和Sigmoid激活函数做了实例，首先我们就来看看均方差+Sigmoid的组合有什么问题。</p><p>首先我们回顾下Sigmoid激活函数的表达式为：<br>$$<br>\sigma(z) = \frac{1}{1+e^{-z}}<br>$$<br>$\sigma(z)$的函数图像如下：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fp5906d8ijj30ci08cjrj.jpg" alt=""></p><p>从图上可以看出，对于Sigmoid，当$z$的取值越来越大后，函数曲线变得越来越平缓，意味着此时的导数$\sigma ^{‘}(z)$也越来越小。同样的，当$z$的取值越来越小时，也有这个问题。仅仅在$z$取值为0附近时，导数$\sigma ^{‘}(z)$的取值较大。</p><p>在上篇讲的均方差+Sigmoid的反向传播算法中，每一层向前递推都要乘以$\sigma ^{‘}(z)$,得到梯度变化值。Sigmoid的这个曲线意味着在大多数时候，我们的梯度变化值很小，导致我们的$W,b$更新到极值的速度较慢，也就是我们的算法收敛速度较慢。那么有什么什么办法可以改进呢？</p><h3 id="2-使用交叉熵损失函数-Sigmoid激活函数改进DNN算法收敛速度"><a href="#2-使用交叉熵损失函数-Sigmoid激活函数改进DNN算法收敛速度" class="headerlink" title="2. 使用交叉熵损失函数+Sigmoid激活函数改进DNN算法收敛速度"></a>2. 使用交叉熵损失函数+Sigmoid激活函数改进DNN算法收敛速度</h3><p>上一节我们讲到Sigmoid的函数特性导致反向传播算法收敛速度慢的问题，那么如何改进呢？换掉Sigmoid？这当然是一种选择。另一种常见的选择是用交叉熵损失函数来代替均方差损失函数。</p><p>我们来看看每个样本的交叉熵损失函数的形式：<br>$$<br>J(W,b,a,y) = -y \bullet lna- (1-y) \bullet ln(1 -a)<br>$$<br>其中，$\bullet$为向量内积。这个形式其实很熟悉，在<a href="http://pancakeawesome.ink/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%8E%9F%E7%90%86%E5%B0%8F%E7%BB%93.html">逻辑回归原理小结</a> 中其实我们就用到了类似的形式，只是当时我们是用最大似然估计推导出来的，而这个损失函数的学名叫交叉熵。</p><p>使用了交叉熵损失函数，就能解决Sigmoid函数导数变化大多数时候反向传播算法慢的问题吗？我们来看看当使用交叉熵时，我们输出层$\delta^L$的梯度情况。</p><p>$$<br> \begin{align} \delta^L &amp; = \frac{\partial J(W,b,a^L,y)}{\partial z^L} \\&amp; = -y\frac{1}{a^L}(a^L)(1-a^L) + (1-y) \frac{1}{1-a^L}(a^L)(1-a^L) \\&amp; = -y(1-a^L) + (1-y)a^L \\&amp; = a^L-y \end{align}<br>$$<br>可见此时我们的$\delta^l$梯度表达式里面已经没有了$\sigma ^{‘}(z)$，作为一个特例，回顾一下我们上一节均方差损失函数时在$\delta^L$梯度，<br>$$<br>\frac{\partial J(W,b,x,y)}{\partial z^L} = (a^L-y) \odot \sigma^{‘}(z)<br>$$<br>对比两者在第L层的$\delta^L$梯度表达式，就可以看出，使用交叉熵，得到的的$\delta^l$梯度表达式没有了$\sigma^{‘}(z)$，梯度为预测值和真实值的差距，这样求得的$W^l,b^l$的地图也不包含$\sigma^{‘}(z)$，因此避免了反向传播收敛速度慢的问题。</p><p>通常情况下，如果我们使用了sigmoid激活函数，交叉熵损失函数肯定比均方差损失函数好用。</p><h3 id="3-使用对数似然损失函数和softmax激活函数进行DNN分类输出"><a href="#3-使用对数似然损失函数和softmax激活函数进行DNN分类输出" class="headerlink" title="3. 使用对数似然损失函数和softmax激活函数进行DNN分类输出"></a>3. 使用对数似然损失函数和softmax激活函数进行DNN分类输出</h3><p>在前面我们讲的所有DNN相关知识中，我们都假设输出是连续可导的值。但是如果是分类问题，那么输出是一个个的类别，那我们怎么用DNN来解决这个问题呢？</p><p>比如假设我们有一个三个类别的分类问题，这样我们的DNN输出层应该有三个神经元，假设第一个神经元对应类别一，第二个对应类别二，第三个对应类别三，这样我们期望的输出应该是(1,0,0)，（0,1,0）和(0,0,1)这三种。即样本真实类别对应的神经元输出应该无限接近或者等于1，而非改样本真实输出对应的神经元的输出应该无限接近或者等于0。或者说，我们希望输出层的神经元对应的输出是若干个概率值，这若干个概率值即我们DNN模型对于输入值对于各类别的输出预测，同时为满足概率模型，这若干个概率值之和应该等于1。</p><p>DNN分类模型要求是输出层神经元输出的值在0到1之间，同时所有输出值之和为1。很明显，现有的普通DNN是无法满足这个要求的。但是我们只需要对现有的全连接DNN稍作改良，即可用于解决分类问题。在现有的DNN模型中，我们可以将输出层第i个神经元的激活函数定义为如下形式：<br>$$<br>a_i^L = \frac{e^{z_i^L}}{\sum\limits_{j=1}^{n_L}e^{z_j^L}}<br>$$<br>其中，$n_L$是输出层第L层的神经元个数，或者说我们的分类问题的类别数。</p><p>很容易看出，所有的$a_i^L$都是在(0,1) 之间的数字，而$\sum\limits_{j=1}^{n_L}e^{z_j^L}$作为归一化因子保证了所有的$a_i^L$之和为1。</p><p>这个方法很简洁漂亮，仅仅只需要将输出层的激活函数从Sigmoid之类的函数转变为上式的激活函数即可。上式这个激活函数就是我们的softmax激活函数。它在分类问题中有广泛的应用。将DNN用于分类问题，在输出层用softmax激活函数也是最常见的了。</p><p>下面这个例子清晰的描述了softmax激活函数在前向传播算法时的使用。假设我们的输出层为三个神经元，而未激活的输出为3,1和-3，我们求出各自的指数表达式为：20,2.7和0.05，我们的归一化因子即为22.75，这样我们就求出了三个类别的概率输出分布为0.88，0.12和0。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fp592w2tf8j30ev08bgmo.jpg" alt=""></p><p>从上面可以看出，将softmax用于前向传播算法是也很简单的。那么在反向传播算法时还简单吗？反向传播的梯度好计算吗？答案是Yes！</p><p>对于用于分类的softmax激活函数，对应的损失函数一般都是用对数似然函数，即：<br>$$<br>J(W,b,a^L,y) = - \sum\limits_ky_klna_k^L<br>$$<br>其中$y_k$的取值为0或者1，如果某一训练样本的输出为第i类。则$y_i=1$,其余的$j \neq i$都有$y_j=0$。由于每个样本只属于一个类别，所以这个对数似然函数可以简化为：<br>$$<br>J(W,b,a^L,y) = -lna_i^L<br>$$<br>其中$i$即为训练样本真实的类别序号。</p><p>可见损失函数只和真实类别对应的输出有关，这样假设真实类别是第i类，则其他不属于第i类序号对应的神经元的梯度导数直接为0。对于真实类别第i类，它的$W_i^L$对应的梯度计算为：<br>$$<br> \begin{align} \frac{\partial J(W,b,a^L,y)}{\partial W_i^L}&amp; = \frac{\partial J(W,b,a^L,y)}{\partial a_i^L}\frac{\partial a_i^L}{\partial z_i^L}\frac{\partial z_i^L}{\partial w_i^L} \\&amp; = -\frac{1}{a_i^L}\frac{(e^{z_i^L})\sum\limits_{j=1}^{n_L}e^{z_j^L}-e^{z_i^L}e^{z_i^L}}{(\sum\limits_{j=1}^{n_L}e^{z_j^L)^2}} a_i^{L-1} \\&amp; = -\frac{1}{a_i^L} (\frac{e^{z_i^L}}{\sum\limits_{j=1}^{n_L}e^{z_j^L}}-\frac{e^{z_i^L}}{\sum\limits_{j=1}^{n_L}e^{z_j^L}}\frac{e^{z_i^L}}{\sum\limits_{j=1}^{n_L}e^{z_j^L}}) a_i^{L-1} \\&amp; = -\frac{1}{a_i^L} a_i^L(1- a_i^L) a_i^{L-1} \\&amp; = (a_i^L -1) a_i^{L-1} \end{align}<br>$$<br>同样的可以得到$b_i^L$的梯度表达式为：<br>$$<br>\frac{\partial J(W,b,a^L,y)}{\partial b_i^L} = a_i^L -1<br>$$<br>可见，梯度计算也很简洁，也没有第一节说的训练速度慢的问题。举个例子，假如我们对于第2类的训练样本，通过前向算法计算的未激活输出为（1,5,3），则我们得到softmax激活后的概率输出为：(0.015,0.866,0.117)。由于我们的类别是第二类，则反向传播的梯度应该为：(0.015,0.866-1,0.117)。是不是很简单呢？</p><p>当softmax输出层的反向传播计算完以后，后面的普通DNN层的反向传播计算和之前讲的普通DNN没有区别。　</p><h3 id="4-梯度爆炸梯度消失与ReLU激活函数"><a href="#4-梯度爆炸梯度消失与ReLU激活函数" class="headerlink" title="4. 梯度爆炸梯度消失与ReLU激活函数"></a>4. 梯度爆炸梯度消失与ReLU激活函数</h3><p>学习DNN，大家一定听说过梯度爆炸和梯度消失两个词。尤其是梯度消失，是限制DNN与深度学习的一个关键障碍，目前也没有完全攻克。</p><p>什么是梯度爆炸和梯度消失呢？从理论上说都可以写一篇论文出来。不过简单理解，就是在反向传播的算法过程中，由于我们使用了是矩阵求导的链式法则，有一大串连乘，如果连乘的数字在每层都是小于1的，则梯度越往前乘越小，导致梯度消失，而如果连乘的数字在每层都是大于1的，则梯度越往前乘越大，导致梯度爆炸。</p><p>比如我们在前一篇反向传播算法里面讲到了$\delta$的计算，可以表示为：<br>$$<br>\delta^l =\frac{\partial J(W,b,x,y)}{\partial z^l} = \frac{\partial J(W,b,x,y)}{\partial z^L}\frac{\partial z^L}{\partial z^{L-1}}\frac{\partial z^{L-1}}{\partial z^{L-2}}…\frac{\partial z^{l+1}}{\partial z^{l}}<br>$$<br>如果不巧我们的样本导致每一层$\frac{\partial z^{l+1}}{\partial z^{l}}$的都小于1，则随着反向传播算法的进行，我们的$\delta^l$会随着层数越来越小，甚至接近越0，导致梯度几乎消失，进而导致前面的隐藏层的$W,b$参数随着迭代的进行，几乎没有大的改变，更谈不上收敛了。这个问题目前没有完美的解决办法。</p><p>而对于梯度爆炸，则一般可以通过调整我们DNN模型中的初始化参数得以解决。</p><p>对于无法完美解决的梯度消失问题，目前有很多研究，一个可能部分解决梯度消失问题的办法是使用ReLU（Rectified Linear Unit）激活函数，ReLU在卷积神经网络CNN中得到了广泛的应用，在CNN中梯度消失似乎不再是问题。那么它是什么样子呢？其实很简单，比我们前面提到的所有激活函数都简单，表达式为：<br>$$<br>\sigma(z) = max(0,z)<br>$$<br>也就是说大于等于0则不变，小于0则激活后为0。就这么一玩意就可以解决梯度消失？至少部分是的。具体的原因现在其实也没有从理论上得以证明。这里我也就不多说了。</p><h3 id="5-DNN其他激活函数"><a href="#5-DNN其他激活函数" class="headerlink" title="5. DNN其他激活函数"></a>5. DNN其他激活函数</h3><p>除了上面提到了激活函数，DNN常用的激活函数还有：</p><p>1） tanh：这个是sigmoid的变种，表达式为：<br>$$<br>tanh(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}<br>$$<br>tanh激活函数和sigmoid激活函数的关系为：<br>$$<br>tanh(z) = 2sigmoid(2z)-1<br>$$<br>tanh和sigmoid对比主要的特点是它的输出落在了[-1,1],这样输出可以进行标准化。同时tanh的曲线在较大时变得平坦的幅度没有sigmoid那么大，这样求梯度变化值有一些优势。当然，要说tanh一定比sigmoid好倒不一定，还是要具体问题具体分析。</p><p>2） softplus：这个其实就是sigmoid函数的原函数，表达式为：<br>$$<br>softplus(z) = log(1+e^z)<br>$$<br>它的导数就是sigmoid函数。softplus的函数图像和ReLU有些类似。它出现的比ReLU早，可以视为ReLU的鼻祖。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fp5955e2okj30et0b93yj.jpg" alt=""></p><p>3）PReLU：从名字就可以看出它是ReLU的变种，特点是如果未激活值小于0，不是简单粗暴的直接变为0，而是进行一定幅度的缩小。如下图。当然，由于ReLU的成功，有很多的跟风者，有其他各种变种ReLU，这里就不多提了。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fp595qupc0j30ey09b3yy.jpg" alt=""></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><h3 id="DNN损失函数和激活函数小结"><a href="#DNN损失函数和激活函数小结" class="headerlink" title="DNN损失函数和激活函数小结"></a>DNN损失函数和激活函数小结</h3><p>上面我们对DNN损失函数和激活函数做了详细的讨论，重要的点有：1）如果使用sigmoid激活函数，则交叉熵损失函数一般肯定比均方差损失函数好。2）如果是DNN用于分类，则一般在输出层使用softmax激活函数和对数似然损失函数。3）ReLU激活函数对梯度消失问题有一定程度的解决，尤其是在CNN模型中。</p><p>下一篇我们讨论下DNN模型的正则化问题。</p><h3 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h3><p>1） <a href="http://neuralnetworksanddeeplearning.com/index.html" target="_blank" rel="noopener">Neural Networks and Deep Learning</a> by By Michael Nielsen</p><p>2） <a href="http://www.deeplearningbook.org/" target="_blank" rel="noopener">Deep Learning</a>, book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville</p><p>3） <a href="http://ufldl.stanford.edu/tutorial/" target="_blank" rel="noopener">UFLDL Tutorial</a></p><blockquote style="margin:2em 0 0;padding:.5em 1em;border-left:3px solid #f44336;background-color:#f5f5f5;list-style:none"><p> <strong>This blog is under a <a href="https://pancakeawesome.ink/" target="_blank">CC BY-NC-SA 3.0 Unported License</a></strong><br> <strong>本文链接：</strong><a href="http://pancakeawesome.ink/深度神经网络损失函数和激活函数的选择.html">http://pancakeawesome.ink/深度神经网络损失函数和激活函数的选择.html</a></p></blockquote></div><div id="changyan-comment"><div id="SOHUCS" sid="深度神经网络损失函数和激活函数的选择.html"></div><script type="text/javascript">!function(){var t="da223e602e0abf03eded8d4c19e7d862";if((window.innerWidth||document.documentElement.clientWidth)<960)window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=cytdf2o7Q&conf='+t+'"><\/script>');else{!function(t,e){var n=document.getElementsByTagName("head")[0]||document.head||document.documentElement,a=document.createElement("script");a.setAttribute("type","text/javascript"),a.setAttribute("charset","UTF-8"),a.setAttribute("src",t),"function"==typeof e&&(window.attachEvent?a.onreadystatechange=function(){var t=a.readyState;"loaded"!==t&&"complete"!==t||(a.onreadystatechange=null,e())}:a.onload=e),n.appendChild(a)}("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:"cytdf2o7Q",conf:t})})}}()</script></div><style>#changyan-comment{background-color:#eee;padding:2pc}</style></div><nav class="material-nav mdl-color-text--grey-50 mdl-cell mdl-cell--12-col"> <a href="/深度神经网络的正则化.html" id="post_nav-newer" class="prev-content"><button class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon mdl-color--white mdl-color-text--grey-900" role="presentation"> <i class="material-icons">arrow_back</i></button> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 新篇</a><div class="section-spacer"></div> <a href="/深度神经网络(DNN)与反向传播算法(BP).html" id="post_nav-older" class="next-content">旧篇 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <button class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon mdl-color--white mdl-color-text--grey-900" role="presentation"> <i class="material-icons">arrow_forward</i></button></a></nav></div></div><div class="sidebar-overlay"></div><aside id="sidebar" class="sidebar sidebar-colored sidebar-fixed-left" role="navigation"><div id="sidebar-main"><div class="sidebar-header header-cover" style="background-image:url(/img/thumbnail/sidebar_header.jpg)"><div class="top-bar"></div> <button type="button" class="sidebar-toggle mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon" style="display:initial" data-upgraded=",MaterialButton,MaterialRipple"> <i class="material-icons">clear_all</i><span class="mdl-button__ripple-container"><span class="mdl-ripple"></span></span></button><div class="sidebar-image"> <img src="/img/rip.jpeg" alt="Edward Guan's avatar"></div> <a data-toggle="dropdown" class="sidebar-brand" href="#settings-dropdown">guanchao930908@163.com<b class="caret"></b></a></div><ul class="nav sidebar-nav"><li class="dropdown"><ul id="settings-dropdown" class="dropdown-menu"><li> <a href="mailto:guanchao930908@163.com" target="_blank" title="Email Me"><i class="material-icons sidebar-material-icons sidebar-indent-left1pc-element">email</i> Email Me</a></li></ul></li><li id="sidebar-first-li"> <a href="/"><i class="material-icons sidebar-material-icons">home</i> 主页</a></li><li class="dropdown"> <a href="#" class="ripple-effect dropdown-toggle" data-toggle="dropdown"><i class="material-icons sidebar-material-icons">timeline</i> 归档<b class="caret"></b></a><ul class="dropdown-menu"><li> <a class="sidebar_archives-link" href="/archives/2018/07/">七月 2018<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/06/">六月 2018<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/05/">五月 2018<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/04/">四月 2018<span class="sidebar_archives-count">14</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/03/">三月 2018<span class="sidebar_archives-count">6</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/02/">二月 2018<span class="sidebar_archives-count">19</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/01/">一月 2018<span class="sidebar_archives-count">6</span></a></li><li><a class="sidebar_archives-link" href="/archives/2017/12/">十二月 2017<span class="sidebar_archives-count">13</span></a></li><li><a class="sidebar_archives-link" href="/archives/2017/11/">十一月 2017<span class="sidebar_archives-count">20</span></a></li><li><a class="sidebar_archives-link" href="/archives/2017/10/">十月 2017<span class="sidebar_archives-count">8</span></a></li><li><a class="sidebar_archives-link" href="/archives/2017/09/">九月 2017<span class="sidebar_archives-count">10</span></a></li></ul></li><li class="dropdown"> <a href="#" class="ripple-effect dropdown-toggle" data-toggle="dropdown"><i class="material-icons sidebar-material-icons">chrome_reader_mode</i> 分类<b class="caret"></b></a><ul class="dropdown-menu"><li> <a class="sidebar_archives-link" href="/categories/OCR/">OCR<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/categories/blog/">blog<span class="sidebar_archives-count">5</span></a></li><li><a class="sidebar_archives-link" href="/categories/javascript/">javascript<span class="sidebar_archives-count">12</span></a></li><li><a class="sidebar_archives-link" href="/categories/nodejs/">nodejs<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/categories/react/">react<span class="sidebar_archives-count">7</span></a></li><li><a class="sidebar_archives-link" href="/categories/前端工具/">前端工具<span class="sidebar_archives-count">4</span></a></li><li><a class="sidebar_archives-link" href="/categories/前端技术/">前端技术<span class="sidebar_archives-count">20</span></a></li><li><a class="sidebar_archives-link" href="/categories/数据分析/">数据分析<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/categories/数据可视化/">数据可视化<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/categories/机器学习/">机器学习<span class="sidebar_archives-count">25</span></a></li><li><a class="sidebar_archives-link" href="/categories/深度学习/">深度学习<span class="sidebar_archives-count">10</span></a></li><li><a class="sidebar_archives-link" href="/categories/目标检测/">目标检测<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/categories/算法思想/">算法思想<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/categories/计算机网络/">计算机网络<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/categories/论文笔记/">论文笔记<span class="sidebar_archives-count">7</span></a></li><li><a class="sidebar_archives-link" href="/categories/软件开发/">软件开发<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/categories/面试/">面试<span class="sidebar_archives-count">1</span></a></li></ul></li><li> <a href="/tags" title="标签云"><i class="material-icons sidebar-material-icons">cloud_circle</i> 标签云</a></li><li class="divider"></li><li> <a href="/timeline" title="Timeline"><i class="material-icons sidebar-material-icons">send</i> Timeline</a></li><li> <a href="/gallery" title="Gallery"><i class="material-icons sidebar-material-icons">photo_library</i> Gallery</a></li><li> <a href="/aboutMe" title="About Me"><i class="material-icons sidebar-material-icons">person_pin</i> About Me</a></li><li class="divider"></li><li> <a href="/archives">文章总数 <span class="sidebar-badge">101</span></a></li></ul></div></aside><div id="back-to-top" class="toTop-wrap"> <a href="#top" class="toTop"><i class="material-icons footer_top-i">expand_less</i></a></div><footer class="mdl-mini-footer" id="bottom"><div class="mdl-mini-footer--left-section sns-list"> <a href="https://twitter.com/591153977" target="_blank"><button class="mdl-mini-footer--social-btn social-btn footer-sns-twitter"> <span class="visuallyhidden">Twitter</span></button></a> <a href="https://www.facebook.com/profile.php?id=100011433530812" target="_blank"><button class="mdl-mini-footer--social-btn social-btn footer-sns-facebook"> <span class="visuallyhidden">Facebook</span></button></a> <a href="https://www.instagram.com/edward930908/" target="_blank"><button class="mdl-mini-footer--social-btn social-btn footer-sns-instagram"> <span class="visuallyhidden">Instagram</span></button></a> <a href="https://github.com/PancakeAwesome" target="_blank"><button class="mdl-mini-footer--social-btn social-btn footer-sns-github"> <span class="visuallyhidden">Github</span></button></a> <a href="https://www.linkedin.com/in/%E8%B6%85-%E7%AE%A1-89495a145/" target="_blank"><button class="mdl-mini-footer--social-btn social-btn footer-sns-linkedin"> <span class="visuallyhidden">LinkedIn</span></button></a></div><div id="copyright"> Copyright&nbsp;©<script type="text/javascript">var fd=new Date;document.write("&nbsp;"+fd.getFullYear()+"&nbsp;")</script>Edward's Blog<br><p>Hosted by <a href="https://pages.coding.me" style="font-weight:700">Coding Pages</a></p></div><div class="mdl-mini-footer--right-section"><div><div class="footer-develop-div">Powered by <a href="https://hexo.io" target="_blank" class="footer-develop-a">Hexo</a></div><div class="footer-develop-div">Theme - <a href="https://github.com/viosey/hexo-theme-material" target="_blank" class="footer-develop-a">Material</a></div></div></div></footer><script>lsloader.load("lazyload_js","/js/lazyload.min.js?1BcfzuNXqV+ntF6gq+5X3Q==",!0)</script><script>lsloader.load("js_js","/js/js.min.js?V/53wGualMuiPM3xoetD5Q==",!0)</script><script>lsloader.load("np_js","/js/nprogress.js?pl3Qhb9lvqR1FlyLUna1Yw==",!0)</script><script type="text/ls-javascript" id="NProgress-script">
    NProgress.configure({
        showSpinner: true
    });
    NProgress.start();
    $('#nprogress .bar').css({
        'background': '#29d'
    });
    $('#nprogress .peg').css({
        'box-shadow': '0 0 10px #29d, 0 0 15px #29d'
    });
    $('#nprogress .spinner-icon').css({
        'border-top-color': '#29d',
        'border-left-color': '#29d'
    });
    setTimeout(function() {
        NProgress.done();
        $('.fade').removeClass('out');
    }, 800);
</script><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><script id="cy_cmt_num" src="https://changyan.sohu.com/upload/plugins/plugins.list.count.js?clientId=cytdf2o7Q"></script><script>var agent=navigator.userAgent.toLowerCase();agent.indexOf("ucbrowser")>0&&(document.write('<link rel="stylesheet" href="/css/uc.css">'),alert("由于 UC 浏览器使用极旧的内核，而本网站使用了一些新的特性。\n为了您能更好的浏览，推荐使用 Chrome 或 Firefox 浏览器。"))</script><script>lsloader.load("prettify_js","/js/prettify.min.js?WN07fivHQSMKWy7BmHBB6w==",!0)</script><script type="text/ls-javascript" id="window-load">
    $(window).on('load', function() {
        // Post_Toc parent position fixed
        $('.post-toc-wrap').parent('.mdl-menu__container').css('position', 'fixed');
    });

    
        
            $(function() {
                $('pre').addClass('prettyprint linenums').attr('style', 'overflow:auto;');
                prettyPrint();
                })
        
    
    
</script><script type="text/ls-javascript" id="lazy-load">
    // Offer LazyLoad
    queue.offer(function(){
        $('.lazy').lazyload({
            effect : 'show'
        });
    });

    // Start Queue
    $(document).ready(function(){
        setInterval(function(){
            queue.execNext();
        },200);
    });
</script><script>!function(){for(var e=document.querySelectorAll('script[type="text/ls-javascript"]'),r=0;r<e.length;++r){var o=e[r];lsloader.runInlineScript(o.id,o.id)}}(),console.log("\n %c © Material Theme | Version: 1.5.0 | https://github.com/viosey/hexo-theme-material %c \n","color:#455a64;background:#e0e0e0;padding:5px 0;border-top-left-radius:5px;border-bottom-left-radius:5px;","color:#455a64;background:#e0e0e0;padding:5px 0;border-top-right-radius:5px;border-bottom-right-radius:5px;")</script></main></div><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>